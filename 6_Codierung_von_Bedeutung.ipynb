{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7fd7b1-7402-4ba1-b4f2-48e964f81dc6",
   "metadata": {},
   "source": [
    "# Die Codierung von Bedeutung\n",
    "\n",
    "<font color=\"blue\">\n",
    "\n",
    "#### Übersicht\n",
    "\n",
    "1. [x] Die Bibliothek von Babel\n",
    "2. [x] Brion Gysin - Permutationen   \n",
    "3. [x] Markov-Ketten\n",
    "4. [x] Transformer-Netze\n",
    "5. [x] OpenAI Codex\n",
    "6. [ ] Die Codierung von Bedeutung - WordVecs und Positional Endcoder\n",
    "7. [ ] Verstehen & Algorithmus\n",
    "8. [ ] Bias\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d0244-0820-430e-8ce6-bbd2f2a76ee8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<img src=\"data/Transformer.png\" width = 500>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5bc9be-1512-4dc0-b05f-ac98f7641694",
   "metadata": {},
   "source": [
    "\n",
    "In natural language processing (NLP), **word embedding** is a term used for the representation of words for text analysis, typically in the form of a **real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning**. (Wikipedia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def60ab2-7949-4352-b7bc-000f7a7d6683",
   "metadata": {},
   "source": [
    "**Pretrained Word embeddings**\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc873c4-bf54-4ab0-ab78-c8814a227ab8",
   "metadata": {},
   "source": [
    "**Word Embeddings: Die Algorithmen**\n",
    "\n",
    "Alle derzeitigen Algorithmen beruhen auf den folgenden Prinzipien:\n",
    "\n",
    "Semantisch ähnliche Wörter werden auf nahe gelegene Punkte abgebildet.\n",
    "Die Grundidee ist die Verteilungshypothese: Wörter, die in denselben Kontexten vorkommen, haben eine gemeinsame semantische Bedeutung wie Tee und Kaffee.<br>\n",
    "\n",
    "\n",
    "<img src= \"data/Word_embeddings_vectors.png\" width=600>\n",
    "\n",
    "<br>\n",
    "Die gebräuchlichsten Algorithmen sind Word2Vec (Mikolov et al. 2013 Google) und GloVe (2014 Stanford), die als Eingabe einen großen Textkorpus nehmen und einen Vektorraum mit typischerweise 100-300 Dimensionen erzeugen. Die entsprechenden Worteinbettungen für die Wörter Kaffee, Tee und Laptop würden **für einen 24-dimensionalen Vektor** beispielsweise wie folgt aussehen:<br>\n",
    "\n",
    "\n",
    "<img src= \"data/Word_embeddings_colored.png\">\n",
    "\n",
    "\n",
    "Images By Predictive Hacks \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b194c-1477-47a0-85ff-458ff3708008",
   "metadata": {},
   "source": [
    "**Self-Attention**\n",
    "\n",
    "In neuronalen Netzen ist \"Attention\" eine Technik, die die kognitive Aufmerksamkeit nachahmt. Einige Teile der Eingabedaten werden verstärkt, während andere Teile abgeschwächt werden. Die Idee dahinter ist, dass das Netzwerk sich mehr auf diesen kleinen, aber wichtigen Teil der Daten konzentrieren sollte.\n",
    "<br>\n",
    "\n",
    "<img src= \"data/Multi-headed-Attention.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71251a-6681-4dbd-aa8c-1eca58c2c20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
